import os
import torch
import torchaudio
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import glob

# --- –ö–∞—Å—Ç–æ–º–Ω–∏–π Dataset –¥–ª—è –≤–∞—à–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ ---
class CustomSpeechCommands(Dataset):
    def __init__(self, data_dir, classes, subset='training'):
        self.data_dir = data_dir
        self.classes = classes
        self.subset = subset
        self.filepaths = []
        self.labels = []
        
        # –®–ª—è—Ö –¥–æ –≤–∞—à–∏—Ö –¥–∞–Ω–∏—Ö
        base_path = os.path.join(data_dir, 'SpeechCommands', 'speech_commands_v0.02')
        
        for class_name in classes:
            # –®—É–∫–∞—î–º–æ –≤—Å—ñ –∞—É–¥—ñ–æ —Ñ–∞–π–ª–∏ –¥–ª—è —Ü—å–æ–≥–æ –∫–ª–∞—Å—É
            pattern = os.path.join(base_path, class_name, '*.wav')
            files = glob.glob(pattern)
            
            for file_path in files:
                self.filepaths.append(file_path)
                self.labels.append(class_name)
        
        print(f"üìÅ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.filepaths)} —Ñ–∞–π–ª—ñ–≤ –¥–ª—è {subset}")
    
    def __len__(self):
        return len(self.filepaths)
    
    def __getitem__(self, idx):
        file_path = self.filepaths[idx]
        label = self.labels[idx]
        
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ —Ñ–∞–π–ª
            waveform, sample_rate = torchaudio.load(file_path)
            return waveform, sample_rate, label, "speaker_0", 0
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {file_path}: {e}")
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ dummy data —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
            dummy_audio = torch.zeros(1, 16000)
            return dummy_audio, 16000, label, "speaker_0", 0

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ ---
target_classes = ['yes', 'no', 'up', 'down']
num_classes = len(target_classes)
batch_size = 16  # –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è CI/CD
epochs = 2
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è ---
mel_spectrogram = torchaudio.transforms.MelSpectrogram(
    sample_rate=16000,
    n_fft=1024,
    hop_length=512,
    n_mels=64
)

def label_to_index(word):
    return torch.tensor(target_classes.index(word))

# --- Collate function ---
def collate_fn(batch):
    tensors, targets = [], []
    
    for waveform, sample_rate, label, speaker_id, utterance_number in batch:
        # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É
        spec = mel_spectrogram(waveform).squeeze(0)  # [64, time]
        tensors.append(spec)
        targets.append(label_to_index(label))
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É –¥–ª—è padding
    max_time = max(spec.shape[1] for spec in tensors)
    
    # Padding –≤—Å—ñ—Ö —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º –¥–æ –æ–¥–Ω–∞–∫–æ–≤–æ—ó –¥–æ–≤–∂–∏–Ω–∏
    padded_tensors = []
    for spec in tensors:
        if spec.shape[1] < max_time:
            pad_size = max_time - spec.shape[1]
            spec = torch.nn.functional.pad(spec, (0, pad_size))
        padded_tensors.append(spec)
    
    return torch.stack(padded_tensors).unsqueeze(1), torch.stack(targets)

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö ---
def get_limited_dataset(subset, samples_per_class=50):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ –≤–∞—à–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –ø–∞–ø–æ–∫"""
    dataset = CustomSpeechCommands('./data', target_classes, subset=subset)
    
    # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    if samples_per_class < len(dataset):
        # –ü—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º–æ –ø–µ—Ä—à—ñ N –∑—Ä–∞–∑–∫—ñ–≤
        indices = list(range(min(samples_per_class * len(target_classes), len(dataset))))
        from torch.utils.data import Subset
        return Subset(dataset, indices)
    
    return dataset

print("Loading datasets...")
train_set = get_limited_dataset('training', samples_per_class=50)
test_set = get_limited_dataset('testing', samples_per_class=20)

train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

print(f"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}")
print(f"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}")

# --- –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ, –∫—Ä–∏—Ç–µ—Ä—ñ—é, –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ ---
model = AudioClassifier(num_classes=num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# --- –¶–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è ---
print("Starting training...")
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        if i % 50 == 49:
            avg_loss = running_loss / 50
            accuracy = 100 * correct / total
            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')
            running_loss = 0.0
            correct = 0
            total = 0

# --- –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ ---
print("Evaluating on test set...")
model.eval()
test_correct = 0
test_total = 0
test_loss = 0.0

with torch.no_grad():
    for i, (inputs, labels) in enumerate(test_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        test_loss += loss.item()
        
        _, predicted = torch.max(outputs.data, 1)
        test_total += labels.size(0)
        test_correct += (predicted == labels).sum().item()

test_accuracy = 100 * test_correct / test_total
avg_test_loss = test_loss / len(test_loader)

print(f'Test Results:')
print(f'Accuracy: {test_accuracy:.2f}%')
print(f'Average Loss: {avg_test_loss:.4f}')

# --- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è ---
torch.save(model.state_dict(), 'model.pth')
print("Model saved to model.pth")

with open('class_info.json', 'w') as f:
    json.dump({
        'target_classes': target_classes
    }, f)

print("Training completed successfully!")
