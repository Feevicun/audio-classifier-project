name: ML Training and Deployment Pipeline

on:
  pull_request:
    branches: [ main, master ]
  
  push:
    branches: [ main, master ]
    paths-ignore:
      - 'README.md'
      - 'docs/**'
      - '*.md'
  
  workflow_dispatch:
    inputs:
      training_epochs:
        description: 'Number of training epochs'
        required: true
        default: '2'
        type: string
      use_minimal_data:
        description: 'Use minimal dataset to save space'
        required: false
        default: true
        type: boolean

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  MODEL_ARTIFACT: trained-model-${{ github.run_id }}

jobs:
  # Job 1: Ð¢Ñ€ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ– (Ð²ÑÐµ Ð² Ð¾Ð´Ð½Ð¾Ð¼Ñƒ job Ð´Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ‚Ð¸)
  train-and-build:
    name: Train Model & Build Images
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      model-version: ${{ steps.version.outputs.model-version }}
      training-status: ${{ steps.train.outputs.training-status }}
      accuracy: ${{ steps.train.outputs.accuracy }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Free disk space
      run: |
        echo "ðŸ§¹ Freeing up disk space..."
        sudo apt-get clean
        docker system prune -f || true
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build training image
      run: |
        echo "ðŸ”¨ Building training image..."
        docker build -f docker/Dockerfile.train -t train-image:latest .
    
    - name: Generate model version
      id: version
      run: |
        TIMESTAMP=$(date +%Y%m%d%H%M%S)
        echo "model-version=model-${TIMESTAMP}-${GITHUB_SHA:0:8}" >> $GITHUB_OUTPUT
    
    - name: Train model
      id: train
      run: |
        echo "ðŸš€ Starting model training..."
        
        # Ð¡Ñ‚Ð²Ð¾Ñ€ÑŽÑ”Ð¼Ð¾ Ð¿Ð°Ð¿ÐºÑƒ Ð´Ð»Ñ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ñ–Ð²
        mkdir -p artifacts
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°Ñ”Ð¼Ð¾ Ñ‚Ñ€ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ
        docker run --rm \
          -v $(pwd)/artifacts:/app/artifacts \
          -v $(pwd)/data:/app/data \
          -e EPOCHS=${{ github.event.inputs.training_epochs || '2' }} \
          -e SAMPLES_PER_CLASS=${{ github.event.inputs.use_minimal_data && '10' || '50' }} \
          train-image:latest
        
        # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸
        if [ -f "artifacts/model.pth" ]; then
          echo "âœ… Model trained successfully"
          echo "training-status=success" >> $GITHUB_OUTPUT
          
          if [ -f "artifacts/training.log" ]; then
            ACCURACY=$(grep "Accuracy:" artifacts/training.log | tail -1 | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "0")
            echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
            echo "ðŸ“Š Final Accuracy: $ACCURACY%"
          fi
        else
          echo "âŒ Model training failed"
          echo "training-status=failed" >> $GITHUB_OUTPUT
          exit 1
        fi
    
    - name: Build inference image
      run: |
        echo "ðŸ”¨ Building inference image..."
        docker build -f docker/Dockerfile.inference -t inference-image:latest .
    
    - name: Upload trained model artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT }}
        path: |
          artifacts/
        retention-days: 30

  # Job 2: Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ–
  model-validation:
    name: Validate Model Quality
    runs-on: ubuntu-latest
    needs: train-and-build
    if: needs.train-and-build.outputs.training-status == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT }}
        path: artifacts/
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pandas scikit-learn
    
    - name: Validate model
      run: |
        echo "ðŸ§ª Validating model..."
        
        python -c "
        import torch
        import json
        import pandas as pd
        from train import AudioClassifier
        import numpy as np
        
        print('Loading model...')
        model = AudioClassifier(num_classes=4)
        model.load_state_dict(torch.load('artifacts/model.pth', map_location='cpu'))
        model.eval()
        
        print('Generating test samples...')
        # Ð¡Ñ‚Ð²Ð¾Ñ€ÑŽÑ”Ð¼Ð¾ ÑÑ‚Ð°Ð±Ñ–Ð»ÑŒÐ½Ñ– Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ– Ð·Ñ€Ð°Ð·ÐºÐ¸
        torch.manual_seed(42)
        test_results = []
        
        for i in range(20):
            # Ð“ÐµÐ½ÐµÑ€ÑƒÑ”Ð¼Ð¾ synthetic audio data
            audio = torch.randn(1, 16000) * 0.1
            
            # Ð Ð¾Ð±Ð¸Ð¼Ð¾ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·
            with torch.no_grad():
                output = model(audio.unsqueeze(0))
                prediction = torch.argmax(output).item()
                confidence = torch.nn.functional.softmax(output, dim=1)[0][prediction].item()
            
            test_results.append({
                'sample_id': i,
                'prediction': prediction,
                'confidence': round(confidence, 4),
                'prediction_label': ['yes', 'no', 'up', 'down'][prediction]
            })
        
        # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸
        validation_report = {
            'total_tests': len(test_results),
            'successful_tests': len([r for r in test_results if r['confidence'] > 0.1]),
            'average_confidence': sum(r['confidence'] for r in test_results) / len(test_results),
            'tests': test_results,
            'model_status': 'VALID' if len(test_results) == 20 else 'INVALID'
        }
        
        with open('artifacts/validation_report.json', 'w') as f:
            json.dump(validation_report, f, indent=2)
        
        # CSV Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸
        df = pd.DataFrame(test_results)
        df.to_csv('artifacts/validation_results.csv', index=False)
        
        print(f'âœ… Validation completed: {validation_report[\"successful_tests\"]}/{validation_report[\"total_tests\"]} tests passed')
        print(f'ðŸ“Š Average confidence: {validation_report[\"average_confidence\"]:.3f}')
        "
    
    - name: Upload validation results
      uses: actions/upload-artifact@v4
      with:
        name: validation-results-${{ github.run_id }}
        path: |
          artifacts/validation_report.json
          artifacts/validation_results.csv
        retention-days: 30

  # Job 3: Benchmark Ñ‚Ð° Ñ‚ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ
  benchmark-and-test:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    needs: [train-and-build, model-validation]
    if: needs.train-and-build.outputs.training-status == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT }}
        path: artifacts/
    
    - name: Build inference image
      run: |
        echo "ðŸ”¨ Building inference image for benchmark..."
        docker build -f docker/Dockerfile.inference -t inference-image:latest .
    
    - name: Run latency benchmark
      run: |
        echo "âš¡ Running performance benchmark..."
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°Ñ”Ð¼Ð¾ ÑÐµÑ€Ð²ÐµÑ€ Ñƒ Ñ„Ð¾Ð½Ñ–
        docker run -d --name benchmark-server -p 5000:5000 inference-image:latest
        
        # Ð§ÐµÐºÐ°Ñ”Ð¼Ð¾ Ð½Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ñ–ÑÑ‚ÑŒ
        echo "â³ Waiting for server..."
        sleep 10
        
        # Ð¢ÐµÑÑ‚ÑƒÑ”Ð¼Ð¾ health endpoint
        python -c "
        import requests
        import time
        import json
        import statistics
        import pandas as pd
        
        print('Starting latency tests...')
        latencies = []
        
        # Ð¢ÐµÑÑ‚ÑƒÑ”Ð¼Ð¾ health endpoint
        for i in range(15):
            start_time = time.time()
            try:
                response = requests.get('http://localhost:5000/health', timeout=10)
                if response.status_code == 200:
                    latency = (time.time() - start_time) * 1000
                    latencies.append(latency)
                    print(f'Request {i+1}: {latency:.2f}ms')
            except Exception as e:
                print(f'Request {i+1} failed: {e}')
        
        # Ð Ð¾Ð·Ñ€Ð°Ñ…Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        if latencies:
            metrics = {
                'total_requests': 15,
                'successful_requests': len(latencies),
                'success_rate': len(latencies) / 15,
                'average_latency_ms': statistics.mean(latencies),
                'min_latency_ms': min(latencies),
                'max_latency_ms': max(latencies),
                'p95_latency_ms': sorted(latencies)[int(len(latencies)*0.95)] if len(latencies) >= 10 else statistics.mean(latencies)
            }
            
            print(f'âœ… Benchmark completed: {metrics[\"success_rate\"]:.1%} success rate')
            print(f'ðŸ“Š Average latency: {metrics[\"average_latency_ms\"]:.2f}ms')
        else:
            metrics = {'error': 'No successful requests'}
            print('âŒ Benchmark failed: no successful requests')
        
        # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸
        with open('benchmark_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ raw data
        if latencies:
            df = pd.DataFrame({'latency_ms': latencies})
            df.to_csv('latency_data.csv', index=False)
        "
        
        # Ð—ÑƒÐ¿Ð¸Ð½ÑÑ”Ð¼Ð¾ ÑÐµÑ€Ð²ÐµÑ€
        docker stop benchmark-server
        docker rm benchmark-server
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: |
          benchmark_metrics.json
          latency_data.csv
        retention-days: 30

  # Job 4: Ð”ÐµÐ¿Ð»Ð¾Ð¹ Ð² Registry (Ñ‚Ñ–Ð»ÑŒÐºÐ¸ Ð´Ð»Ñ main/master)
  deploy-to-registry:
    name: Deploy to GitHub Container Registry
    runs-on: ubuntu-latest
    needs: [train-and-build, model-validation, benchmark-and-test]
    if: |
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'push' && github.ref == 'refs/heads/master')
    
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT }}
        path: artifacts/
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push to GHCR
      run: |
        echo "ðŸš€ Building and pushing to GitHub Container Registry..."
        
        docker build -f docker/Dockerfile.inference \
          -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest \
          -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ needs.train-and-build.outputs.model-version }} .
        
        docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ needs.train-and-build.outputs.model-version }}
        
        echo "âœ… Successfully deployed to GHCR"
        echo "ðŸ“¦ Image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ needs.train-and-build.outputs.model-version }}"

  # Job 5: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ Ð·Ð²Ñ–Ñ‚Ñƒ
  generate-report:
    name: Generate Pipeline Report
    runs-on: ubuntu-latest
    needs: [train-and-build, model-validation, benchmark-and-test]
    if: always()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Generate Markdown report
      run: |
        echo "ðŸ“Š Generating pipeline report..."
        
        cat > pipeline_report.md << EOF
        # ML Pipeline Execution Report
        ## Run Information
        - **Run ID**: ${{ github.run_id }}
        - **Trigger**: ${{ github.event_name }}
        - **Commit**: ${{ github.sha }}
        - **Timestamp**: $(date -u)
        
        ## Job Status
        | Job | Status | Details |
        |-----|--------|---------|
        | Training & Build | ${{ needs.train-and-build.result }} | Accuracy: ${{ needs.train-and-build.outputs.accuracy }}% |
        | Model Validation | ${{ needs.model-validation.result }} | Quality checks completed |
        | Performance Benchmark | ${{ needs.benchmark-and-test.result }} | Latency tests executed |
        | Deployment | ${{ needs.deploy-to-registry.result || 'N/A' }} | ${{ needs.deploy-to-registry.result && 'Deployed to GHCR' || 'Skipped' }} |
        
        ## Artifacts Generated
        - âœ… Trained model (model.pth)
        - âœ… Training logs and metrics
        - âœ… Validation reports (JSON/CSV)
        - âœ… Performance benchmarks (latency metrics)
        - âœ… Docker images
        
        ## Pipeline Quality Gates
        - Training: ${{ needs.train-and-build.outputs.training-status == 'success' && 'âœ… PASS' || 'âŒ FAIL' }}
        - Validation: ${{ needs.model-validation.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }}
        - Benchmark: ${{ needs.benchmark-and-test.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }}
        
        ### Merge Request Status
        ${{ github.event_name == 'pull_request' && 'This MR will be blocked until all checks pass' || 'Push to main branch' }}
        
        EOF
        
        echo "âœ… Report generated"
    
    - name: Upload report
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-report-${{ github.run_id }}
        path: pipeline_report.md
        retention-days: 90

  # Job 6: Status check Ð´Ð»Ñ MR
  status-check:
    name: Required Status Check
    runs-on: ubuntu-latest
    needs: [train-and-build, model-validation, benchmark-and-test]
    if: always() && github.event_name == 'pull_request'
    
    steps:
    - name: Determine MR status
      run: |
        if [ "${{ needs.train-and-build.result }}" = "success" ] && \
           [ "${{ needs.model-validation.result }}" = "success" ] && \
           [ "${{ needs.benchmark-and-test.result }}" = "success" ]; then
          echo "âœ… All checks passed - MR can be merged"
          echo "## âœ… All pipeline checks passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Some checks failed - MR cannot be merged"
          echo "## âŒ Pipeline checks failed - MR blocked" >> $GITHUB_STEP_SUMMARY
          exit 1
        fi