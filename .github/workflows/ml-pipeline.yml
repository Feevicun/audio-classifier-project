name: ML Training and Deployment Pipeline

on:
  push:
    branches: [ main, master ]
    paths-ignore:
      - 'README.md'
      - 'docs/**'
      - '*.md'
  
  pull_request:
    branches: [ main, master ]
  
  workflow_dispatch:
    inputs:
      training_epochs:
        description: 'Number of training epochs'
        required: true
        default: '2'
        type: string
      skip_training:
        description: 'Skip training phase'
        required: false
        default: false
        type: boolean

env:
  REGISTRY: ghcr.io
  IMAGE_NAME_TRAIN: ${{ github.repository }}-train
  IMAGE_NAME_INFERENCE: ${{ github.repository }}-inference
  MODEL_ARTIFACT_NAME: trained-model

jobs:
  # Job 1: Ğ¢Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
  train-model:
    name: Train ML Model
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      model-version: ${{ steps.version.outputs.model-version }}
      training-status: ${{ steps.train.outputs.training-status }}
      accuracy: ${{ steps.train.outputs.accuracy }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Debug repository structure
      run: |
        echo "ğŸ“ Repository structure:"
        find . -name "*.py" -o -name "*.yml" -o -name "Dockerfile*" -o -name "requirements.txt" | head -20
        echo "ğŸ” Data directory:"
        ls -la data/ || echo "No data directory"
        find data/ -name "*.wav" | head -10 || echo "No WAV files found"
    
    - name: Create test data if missing
      run: |
        echo "ğŸ“ Checking data structure..."
        if find data/ -name "*.wav" | head -1 > /dev/null; then
          echo "âœ… Real data found"
          find data/ -name "*.wav" | head -5
        else
          echo "ğŸ“ Creating minimal test data structure..."
          mkdir -p data/SpeechCommands/speech_commands_v0.02
          for class_name in yes no up down; do
            mkdir -p "data/SpeechCommands/speech_commands_v0.02/$class_name"
            echo "Creating placeholder for $class_name"
          done
          echo "âš ï¸ Using synthetic data for training"
        fi
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Generate model version
      id: version
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        COMMIT_SHA_SHORT=${GITHUB_SHA:0:8}
        echo "model-version=v${TIMESTAMP}-${COMMIT_SHA_SHORT}" >> $GITHUB_OUTPUT
        echo "version=${TIMESTAMP}-${COMMIT_SHA_SHORT}" >> $GITHUB_OUTPUT
    
    - name: Build training image
      id: build-train
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.train
        tags: train-image:latest
        load: true
        outputs: type=docker,dest=/tmp/train-image.tar
    
    - name: Load training image
      run: |
        echo "ğŸ“¦ Loading training image..."
        docker load --input /tmp/train-image.tar
        docker images | grep train-image
    
    - name: Train model
      id: train
      run: |
        echo "ğŸš€ Starting model training..."
        
        # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
        mkdir -p models
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ñ‚Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñƒ
        docker run --rm \
          -v $(pwd)/models:/app/models \
          -v $(pwd)/data:/app/data \
          -e EPOCHS=${{ github.event.inputs.training_epochs || '2' }} \
          train-image:latest
        
        # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ñ‚Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ
        if [ -f "models/model.pth" ]; then
          echo "âœ… Model trained successfully"
          echo "training-status=success" >> $GITHUB_OUTPUT
          
          # ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ğ¼Ğ¾ accuracy Ğ· Ğ»Ğ¾Ğ³Ñ–Ğ²
          if [ -f "training.log" ]; then
            ACCURACY=$(grep "Accuracy:" training.log | tail -1 | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "0")
            echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
            echo "ğŸ“Š Final Accuracy: $ACCURACY%"
          else
            echo "accuracy=0" >> $GITHUB_OUTPUT
          fi
        else
          echo "âŒ Model training failed - no model file created"
          echo "training-status=failed" >> $GITHUB_OUTPUT
          echo "accuracy=0" >> $GITHUB_OUTPUT
          exit 1
        fi
    
    - name: Upload trained model and logs
      uses: actions/upload-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT_NAME }}
        path: |
          models/
          class_info.json
          training.log
        retention-days: 30
        if-no-files-found: error

  # Job 2: Ğ‘Ñ–Ğ»Ğ´ inference ÑĞµÑ€Ğ²ĞµÑ€Ğ°
  build-inference:
    name: Build Inference Server
    runs-on: ubuntu-latest
    needs: train-model
    if: needs.train-model.outputs.training-status == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download trained model
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT_NAME }}
        path: ./
    
    - name: Verify model files
      run: |
        echo "ğŸ” Verifying model artifacts..."
        ls -la models/ || echo "No models directory"
        ls -la class_info.json || echo "No class_info.json"
        [ -f "model.pth" ] && echo "âœ… model.pth found" || echo "âŒ model.pth missing"
        [ -f "class_info.json" ] && echo "âœ… class_info.json found" || echo "âŒ class_info.json missing"
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build inference image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.inference
        tags: inference-image:latest
        load: true
        outputs: type=docker,dest=/tmp/inference-image.tar
    
    - name: Save inference image as artifact
      uses: actions/upload-artifact@v4
      with:
        name: inference-image
        path: /tmp/inference-image.tar
        retention-days: 7
        if-no-files-found: error

  # Job 3: Ğ¢ĞµÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
  test-model:
    name: Test Model Quality
    runs-on: ubuntu-latest
    needs: train-model
    if: needs.train-model.outputs.training-status == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download trained model
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT_NAME }}
        path: ./
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-html
    
    - name: Run model tests
      id: tests
      run: |
        echo "ğŸ§ª Running model tests..."
        python -m pytest tests/ -v --html=test-report.html --self-contained-html || echo "âš ï¸ Some tests failed but continuing"
        
        # Check if tests passed
        if [ $? -eq 0 ]; then
          echo "âœ… All tests passed"
          echo "test-status=success" >> $GITHUB_OUTPUT
        else
          echo "âš ï¸ Some tests failed"
          echo "test-status=failed" >> $GITHUB_OUTPUT
        fi
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      with:
        name: test-results
        path: |
          test-report.html
          training.log
        retention-days: 30
        if-no-files-found: warn

  # Job 4: Performance benchmarking
  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    needs: [train-model, build-inference]
    if: needs.train-model.outputs.training-status == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download model and image
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT_NAME }}
        path: ./
    
    - name: Build inference image locally
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.inference
        tags: inference-image:latest
        load: true
        outputs: type=docker,dest=/tmp/inference-image.tar
    
    - name: Load inference image
      run: |
        docker load --input /tmp/inference-image.tar
        docker images | grep inference-image
    
    - name: Install benchmark dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run performance tests
      id: benchmark
      run: |
        echo "âš¡ Starting performance benchmark..."
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ ÑĞµÑ€Ğ²ĞµÑ€ Ñƒ Ñ„Ğ¾Ğ½Ñ– Ğ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñƒ
        docker run -d --name inference-server -p 5000:5000 inference-image:latest
        
        # Ğ§ĞµĞºĞ°Ñ”Ğ¼Ğ¾ Ğ½Ğ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ñ–ÑÑ‚ÑŒ ÑĞµÑ€Ğ²ĞµÑ€Ğ°
        echo "â³ Waiting for server to start..."
        for i in {1..30}; do
          if curl -s -f http://localhost:5000/health > /dev/null; then
            echo "âœ… Server is ready!"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "âŒ Server failed to start"
            docker logs inference-server
            exit 1
          fi
          sleep 2
        done
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº
        echo "ğŸ“Š Running benchmark..."
        python benchmark.py --url http://localhost:5000 --output benchmark-results.json --requests 5
        
        # Ğ—ÑƒĞ¿Ğ¸Ğ½ÑÑ”Ğ¼Ğ¾ ÑĞµÑ€Ğ²ĞµÑ€
        docker stop inference-server
        docker rm inference-server
        
        # Check benchmark results
        if [ -f "benchmark-results.json" ]; then
          SUCCESS_RATE=$(python -c "import json; data=json.load(open('benchmark-results.json')); print(data.get('success_rate', 0))")
          echo "ğŸ“ˆ Benchmark success rate: $SUCCESS_RATE"
          if (( $(echo "$SUCCESS_RATE >= 0.5" | bc -l) )); then
            echo "benchmark-status=success" >> $GITHUB_OUTPUT
          else
            echo "benchmark-status=failed" >> $GITHUB_OUTPUT
          fi
        else
          echo "benchmark-status=failed" >> $GITHUB_OUTPUT
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark-results.json
        retention-days: 30
        if-no-files-found: warn

  # Job 5: Deploy to Registry
  deploy:
    name: Deploy to Registry
    runs-on: ubuntu-latest
    needs: [train-model, build-inference, test-model, benchmark]
    if: |
      github.event_name == 'push' && 
      (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') &&
      needs.train-model.outputs.training-status == 'success'
    
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download model
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.MODEL_ARTIFACT_NAME }}
        path: ./
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push inference image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.inference
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ github.repository }}-inference:latest
          ${{ env.REGISTRY }}/${{ github.repository }}-inference:${{ needs.train-model.outputs.model-version }}
        labels: |
          org.opencontainers.image.source=${{ github.server_url }}/${{ github.repository }}
          org.opencontainers.image.revision=${{ github.sha }}
    
    - name: Update deployment info
      run: |
        echo "ğŸš€ Successfully deployed to GitHub Container Registry"
        echo "ğŸ“¦ Image: ${{ env.REGISTRY }}/${{ github.repository }}-inference:${{ needs.train-model.outputs.model-version }}"
        echo "ğŸ¯ Accuracy: ${{ needs.train-model.outputs.accuracy }}%"

  # Job 6: Generate Report
  report:
    name: Generate Pipeline Report
    runs-on: ubuntu-latest
    needs: [train-model, test-model, benchmark]
    if: always()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Generate Markdown report
      run: |
        cat > pipeline-report.md << EOF
        # ML Pipeline Report
        
        ## Execution Summary
        - **Pipeline ID**: ${{ github.run_id }}
        - **Trigger**: ${{ github.event_name }}
        - **Commit**: ${{ github.sha }}
        - **Timestamp**: $(date -u)
        - **Workflow**: ${{ github.workflow }}
        
        ## Job Status
        | Job | Status | Result |
        |-----|--------|--------|
        | Training | ${{ needs.train-model.result }} | ${{ needs.train-model.outputs.training-status || 'N/A' }} |
        | Testing | ${{ needs.test-model.result }} | ${{ needs.test-model.outputs.test-status || 'N/A' }} |
        | Benchmark | ${{ needs.benchmark.result }} | ${{ needs.benchmark.outputs.benchmark-status || 'N/A' }} |
        
        ## Model Information
        - **Version**: ${{ needs.train-model.outputs.model-version || 'N/A' }}
        - **Accuracy**: ${{ needs.train-model.outputs.accuracy || 'N/A' }}%
        - **Training Status**: ${{ needs.train-model.outputs.training-status || 'N/A' }}
        
        ## Artifacts Generated
        1. **Trained Model** - model.pth and class_info.json
        2. **Test Results** - Unit test report
        3. **Benchmark Results** - Performance metrics
        4. **Docker Image** - Inference server
        
        ## Next Steps
        1. Review test results in artifacts
        2. Check benchmark metrics for performance
        3. Deploy to production if all checks pass
        4. Monitor model performance in production
        
        ### Quality Gates
        - âœ… Training completed: ${{ needs.train-model.outputs.training-status == 'success' && 'PASS' || 'FAIL' }}
        - âœ… Model accuracy: ${{ needs.train-model.outputs.accuracy || '0' }}%
        - âœ… Tests executed: ${{ needs.test-model.result == 'success' && 'PASS' || 'FAIL' }}
        - âœ… Benchmark completed: ${{ needs.benchmark.result == 'success' && 'PASS' || 'FAIL' }}
        
        ### Deployment
        ${{ needs.deploy && needs.deploy.result == 'success' && 'âœ… Successfully deployed to GitHub Container Registry' || 'â³ Not deployed (only on main branch)' }}
        
        EOF
        
        echo "ğŸ“Š Pipeline Report Generated"
        cat pipeline-report.md
    
    - name: Upload pipeline report
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-report
        path: pipeline-report.md
        retention-days: 30

  # Job 7: Notify Status
  notify:
    name: Notify Pipeline Status
    runs-on: ubuntu-latest
    needs: [train-model, test-model, benchmark, report]
    if: always()
    
    steps:
    - name: Pipeline Status Summary
      run: |
        echo "ğŸŠ PIPELINE EXECUTION COMPLETE"
        echo "================================"
        echo "Training: ${{ needs.train-model.result }}"
        echo "Testing: ${{ needs.test-model.result }}" 
        echo "Benchmark: ${{ needs.benchmark.result }}"
        echo "Deployment: ${{ needs.deploy.result || 'N/A' }}"
        echo "================================"
        if [ "${{ needs.train-model.result }}" = "success" ] && [ "${{ needs.test-model.result }}" = "success" ]; then
          echo "âœ… PIPELINE SUCCESS"
          exit 0
        else
          echo "âŒ PIPELINE FAILED"
          exit 1
        fi